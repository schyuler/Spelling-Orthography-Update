{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e089c10",
   "metadata": {},
   "source": [
    "# Spelling Orthography Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8bb52",
   "metadata": {},
   "source": [
    "This project uses machine learning to update text from an older spelling orthography to a modern spelling orthography. The focus on this project will be on updating the spelling orthography from the 1908 Chamorro Bible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2bed1",
   "metadata": {},
   "source": [
    "**Name:** Schyuler Lujan<br>\n",
    "**Date Started:** 6-Nov-2024<br>\n",
    "**Date Completed:** In Progress<br>\n",
    "**Last Updated:** 7-Nov-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5fba6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup # For web scraping\n",
    "import requests # For web scraping\n",
    "import re # For text cleaning\n",
    "import pandas as pd # For analysis\n",
    "import matplotlib.pyplot as plt # For analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a847e9",
   "metadata": {},
   "source": [
    "# Scrape Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87adbe75",
   "metadata": {},
   "source": [
    "Scrape the text data from the chamorrobible.org website and format the text into a dataset of unduplicated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29992ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All text can be found at this URL\n",
    "website = 'http://chamorrobible.org/download/YSantaBiblia-Chamorro-HTML.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76529ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(website)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d89eff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the text\n",
    "ch_bible_text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1b81262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the text\n",
    "#print(ch_bible_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3091dd",
   "metadata": {},
   "source": [
    "# Clean Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f162993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "text_clean = re.sub(r\"\\d+\", \" \", ch_bible_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5928078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "text_clean = re.sub(r\"[^\\w\\s]\", \"\", text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d6dd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize text by converting to lowercase\n",
    "text_clean = text_clean.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e367c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text by word and store in a list of duplicated words\n",
    "total_word_list = text_clean.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c9ad3",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73742877",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e755210e",
   "metadata": {},
   "source": [
    "### Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1995cc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total word count: 132,875\n"
     ]
    }
   ],
   "source": [
    "# Get total word count\n",
    "total_word_count = len(total_word_list)\n",
    "print(f\"The total word count: {total_word_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e21c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique word count: 11,193\n"
     ]
    }
   ],
   "source": [
    "# Get unique word count\n",
    "unique_word_set = set(total_word_list)\n",
    "unique_word_count = len(unique_word_set)\n",
    "print(f\"The unique word count: {unique_word_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebee25",
   "metadata": {},
   "source": [
    "### Word Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31e19421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average word length\n",
    "word_length = [] # Initialize list\n",
    "\n",
    "for word in total_word_list:\n",
    "    word_length.append(len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "808b39c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 4.725388523047977 characters\n"
     ]
    }
   ],
   "source": [
    "average_word_length = sum(word_length) / total_word_count\n",
    "print(f\"Average word length: {average_word_length} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e7ee868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word: 22 characters\n",
      "Shortest word: 1 character\n"
     ]
    }
   ],
   "source": [
    "# Get maximum word length\n",
    "word_length.sort(reverse=True) # Sort in descending order\n",
    "max_word_length = word_length[0]\n",
    "min_word_length = word_length[-1]\n",
    "print(f\"Longest word: {max_word_length} characters\")\n",
    "print(f\"Shortest word: {min_word_length} character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f4029",
   "metadata": {},
   "source": [
    "### Character Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02102232",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = {} # Initialize dictionary for storing characters\n",
    "total_character_count = 0 # For holding the total character count\n",
    "\n",
    "# Character counts\n",
    "for word in total_word_list:\n",
    "    for char in word:\n",
    "        total_character_count += 1\n",
    "        if char in characters:\n",
    "            characters[char] += 1\n",
    "        else:\n",
    "            characters[char] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2085afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of characters in the text: 627,886\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of characters in the text: {total_character_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236eed00",
   "metadata": {},
   "source": [
    "## Frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffb9c5",
   "metadata": {},
   "source": [
    "### Character Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f28f0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to a list of tuples before converting to dataframe\n",
    "character_list = [] # Initialize list\n",
    "for char in characters:\n",
    "    character_list.append((char, characters[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7162185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Character  Frequency\n",
      "2          a     130786\n",
      "3          n      60208\n",
      "9          o      47056\n",
      "6          i      46099\n",
      "14         e      39671\n",
      "0          y      38423\n",
      "12         u      34416\n",
      "15         g      34305\n",
      "1          s      30192\n",
      "4          t      29560\n",
      "10         j      28684\n",
      "8          m      27442\n",
      "7          l      18041\n",
      "17         p       9827\n",
      "11         c       9349\n",
      "13         r       7984\n",
      "19         d       7472\n",
      "18         ñ       7411\n",
      "20         f       6352\n",
      "5          b       4211\n",
      "23         ü       4050\n",
      "16         h       3452\n",
      "25         q       1023\n",
      "22         v        870\n",
      "27         â        646\n",
      "24         á        160\n",
      "26         é         85\n",
      "28         ó         51\n",
      "21         ú         23\n",
      "29         í         18\n",
      "33         x         12\n",
      "30         ô          2\n",
      "31         z          2\n",
      "32         k          2\n",
      "34         ã          1\n"
     ]
    }
   ],
   "source": [
    "# Convert to dataframe and view results in descending order\n",
    "character_frequency_df = pd.DataFrame(character_list, columns=[\"Character\", \"Frequency\"])\n",
    "# Sort dataframe by frequency\n",
    "character_frequency_df.sort_values(by=\"Frequency\", ascending=False, inplace=True)\n",
    "print(character_frequency_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725531a",
   "metadata": {},
   "source": [
    "### Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f387408",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {} # Initialize dictionary to store word counts\n",
    "\n",
    "# Iterate through word list and count each word\n",
    "for word in total_word_list:\n",
    "    if word in words:\n",
    "        words[word] += 1\n",
    "    else:\n",
    "        words[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59a8b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = [] # Initialize list\n",
    "for word in words:\n",
    "    word_frequencies.append((word, words[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a0593bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  Frequency\n",
      "0            y      15477\n",
      "57          ya       7730\n",
      "83          na       4631\n",
      "10         gui       4123\n",
      "4         sija       3631\n",
      "9          yan       2958\n",
      "40          ni       2813\n",
      "15          si       2354\n",
      "181       para       1629\n",
      "41          ti       1487\n",
      "81          sa       1450\n",
      "171      güiya       1242\n",
      "39      taotao       1240\n",
      "216       anae       1117\n",
      "50         ayo       1087\n",
      "129     ilegña       1077\n",
      "130         nu       1061\n",
      "52         lao       1020\n",
      "53       guiya       1013\n",
      "73        todo        965\n",
      "153      jamyo        959\n",
      "183       yuus        848\n",
      "391       este        824\n",
      "239        güe        818\n",
      "137        pot        811\n",
      "55       jeova        749\n",
      "5371     jesus        681\n",
      "119      guajo        668\n",
      "114        nae        662\n",
      "17          un        654\n",
      "185       jago        599\n",
      "182         as        522\n",
      "90        jafa        510\n",
      "97        tano        476\n",
      "150          o        424\n",
      "206         yo        411\n",
      "531      locue        402\n",
      "352       esta        389\n",
      "460     guinin        386\n",
      "103   ilegñija        382\n",
      "60       jaane        364\n",
      "132        jao        364\n",
      "179       taya        344\n",
      "112      señot        330\n",
      "389     yaguin        326\n",
      "766       mato        320\n",
      "68       guaja        313\n",
      "395    magajet        298\n",
      "226       asta        296\n",
      "30         uno        295\n"
     ]
    }
   ],
   "source": [
    "# Convert to a dataframe\n",
    "word_frequencies_df = pd.DataFrame(word_frequencies, columns=[\"Word\", \"Frequency\"])\n",
    "word_frequencies_df.sort_values(by=\"Frequency\", ascending=False, inplace=True)\n",
    "# View top 100 words\n",
    "print(word_frequencies_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9441b",
   "metadata": {},
   "source": [
    "### Lexical Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ee2ab",
   "metadata": {},
   "source": [
    "Assess how many words are used more than once, verses the number of words used only one time in the entire text to understand the diversity of the text vs. amount of repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63cd8406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words occuring once: 6,207\n",
      "Unique words are 0.5545430179576522 of the dataset\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe of non-repeated words\n",
    "not_repeated_df = word_frequencies_df[word_frequencies_df[\"Frequency\"] == 1]\n",
    "\n",
    "# Count the number of words ocurring only once\n",
    "total_not_repeated = len(not_repeated_df)\n",
    "print(f\"Total words occuring once: {total_not_repeated:,}\")\n",
    "print(f\"Unique words are {(total_not_repeated / unique_word_count)} of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98c3d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words repeated: 4,986\n",
      "Repeated words are 0.4454569820423479 of the dataset\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe of repeated words\n",
    "repeated_words_df = word_frequencies_df[word_frequencies_df[\"Frequency\"] > 1]\n",
    "# Count the number of repeated words\n",
    "total_repeated = len(repeated_words_df)\n",
    "print(f\"Total words repeated: {total_repeated:,}\")\n",
    "print(f\"Repeated words are {total_repeated / unique_word_count} of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d66fd152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     4986.000000\n",
       "mean        25.404733\n",
       "std        283.286989\n",
       "min          2.000000\n",
       "25%          2.000000\n",
       "50%          4.000000\n",
       "75%          8.000000\n",
       "max      15477.000000\n",
       "Name: Frequency, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get descriptive statistics for Frequency on repeated words\n",
    "repeated_words_df[\"Frequency\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f7309",
   "metadata": {},
   "source": [
    "# Export Data to Create Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23b59f2",
   "metadata": {},
   "source": [
    "Currently, there is no labelled dataset available to train our models. Therefore, I will need to manually create a dataset of sample pairs for training these models. The dataset will come from the unique word set, and the chosen words will be mapped to their new orthography equivalent.<p>\n",
    "    \n",
    "**Size of Sample** <br>\n",
    "    \n",
    "I will start with 10% of the total unique words in the text. Although more data is always better, since I must manually create the labelled training set, I will start with 10% to minimize unnecessary, manual work.<p>\n",
    "    \n",
    "**Creating a Representative Sample**<br>\n",
    "\n",
    "To ensure a representative sample of words in our training set, I will create a stratified random sample by word frequency. **(I will include more notes from the lexical diversity analysis.)** The aim is to prevent over-fitting based upon the highest frequency words and still capture unique words, while also ensuring that the model will perform well on high frequency words. To create this stratified random sample, I will divide the unique word dataset into different groups based on word frequency. Then I will randomly sample from these groups to create our entire sample set. <p>\n",
    "    \n",
    "**Considerations for Poor Model Performance**\n",
    "    \n",
    "There is the possibility that the models will perform poorly on this manually created training set. Since the resource cost of increasing the sample training set is so high, I will only increase the sample size incrementally. One approach is to identify the model's lowest confidence words, detect any common patterns in these words (ie: specific affixes) and include more similarly affixed words in the sample set to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76762b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export word frequency dataframe to CSV file, for additional reference\n",
    "#word_frequencies_df.to_csv('chamorro_bible_words.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of words needed for 10% of unique words\n",
    "sample_size = int(.1 * len(unique_word_set))\n",
    "print(f\"Sample size: {sample_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea14b3f",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6633ba9",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52ddc7",
   "metadata": {},
   "source": [
    "# Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d6b91",
   "metadata": {},
   "source": [
    "# Export Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8fe3ce",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6b282",
   "metadata": {},
   "source": [
    "# Opportunities for Future Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
